---
title: "Predictive Modeling of Used Car Prices with Statistical Linear Models"
subtitle: "Mth 439 Final Project"
author: "Haris Naveed, Griffin Lovato, Andrew Nahlik, Christian Calian, Jack Weitzner"
date: "2023-11-19"
output: pdf_document
---

## Data Cleanup:

First, we have to load in our data. The head of the data can be seen below: Be sure to download the file to the correct location on your machine or the connection will not open properly...

```{r, echo=FALSE}
raw_data = read.csv("used_cars_UK.csv", fileEncoding = "UTF-8")
head(raw_data)
```

Above, we can see what our raw data table looks like. The first step in cleaning this data is to encode the "Engine" column, currently considered a char, into a numeric variable. We will later revisit whether this categorical to quantitative encoding is valid. As we are focusing on continuous variables for this project, we will remove columns: "title", "Fuel.type", "Gearbox", "Emission Class", and "Service.history". The categorical variable "Body.type" will remain in our data set as a useful variable for stratification. We can also rename the columns to have a standardized form. Finally, we can remove any rows with NA values.

```{r, echo=FALSE}
data = raw_data[, c("Price", "Mileage.miles.", "Registration_Year", "Previous.Owners", "Body.type", "Engine", "Doors", "Seats")]
data$Engine = as.numeric(gsub("L", "", raw_data$Engine))
colnames(data)[colnames(data) == "Mileage.miles."] = "Mileage"
colnames(data)[colnames(data) == "Previous.Owners"] = "Previous_Owners"
colnames(data)[colnames(data) == "Body.type"] = "Body_Type"
data = na.omit(data)
head(data)
```

## Initial Data Analysis:

### Quick Summary:

A quick way to investigate the properties of our data set is to use the summary function:

```{r, echo=FALSE}
summary(data)
```

--- Comments about above summary ---

Based on the summary we can establish a few baseline assumptions about the data.

#### Price:

Beginning with price, the spread of the quartiles suggests that most of the vehicles are below \$6000 with the 3rd quartile bieng at \$8491. The maximum value being \$33900 suggests however that we are likely to have outliers in terms of price. This could be explained by luxury or collectible vintage vehicles.

#### Mileage:

The mileage seems like a relatively normal distriution. The median and mean values are approximately 81,000 with the first quartile being approximately 60,000 and the 3rd quartile being around 105000. These are both slightly more than 20,000 from the mean/median which would suggest a good distribution. The minimum value in mileage does give us a hint on price. A minimum value of mileage being 6 suggests that in this dataset there are essentially new cars. In considering building a model these "nearly new" should probably be removed from the model.

#### Registration Year:

Registration year is likely to be biased toward more modern years (right skewed distribution). We see a minimum datapoint in 1987, but 75% of the datapoints are for vehicles in the last 15 years (2009 or newer).

#### Previous Owners:

This is a long tailed distribution. From a practical knowledge perspective it is suprising to see even one vehicle has had 9 previous owners and is now on sale for a tenth time. Either this is a really old vehicle with a lot of miles or there is something wrong with it that makes it undesirable. It is also possible that this is an error in the data.

#### Body Type:

This is a categorical variable which the summarry() function cannot properly view. During later processing we can view this as a histogram and better understand what different categories these vehicles fall into.

#### Engine:

This is the engine displacement (eg. 2L engine). It is curious that the median and mean value for this parameter are 1.6L as this strikes us as somewhat low. That said, this dataset is for cars registered for sale in the UK which often has smaller and somewhat less powerful vehicles compared to the US. The maximum value is likely to be an outlier as a sportscar or something similar.

#### Doors:

This variable is likely somewhat difficult to categorize. A large amount of the vehicles (at least 50%) claim to have 5 doors, however this does not seem possible logically unless the trunk is bieng counted as a 'door'. If this is the case 5 door vehicles should likely be relabeled as 4 door vehicles. Most vehicles are likely to have a trunk and it intutively makes sense to have an even number of doors per vehicle.

#### Seats:

The distribution of the quartiles of seats suggests that the data is very short tailed and clustered around the 5 seats that we would expect in a standard sedan or SUV. Viewing the histogram of this variable should be able to help significantly. More analysis to come in the next section.

### Analysis of Histograms of Variables:

Additionally, we can investigate the histograms of each of our variables:

```{r, echo=FALSE}
par(mfrow = c(2, 2)) 
for (col in names(data)) {
  if (is.numeric(data[[col]])) {
    hist(data[[col]], main = paste("Histogram of", col), xlab= col)
  }
}
```

--- Comments about above histograms---

Viewing the Histograms helps to clarify some of the assumptions made in the summary phase. While a variable by variable break down is likely repetitious covering a few of the histograms is helpful. Price, Mileage, Registration Year, and engine all have suspected outlier values which distort the histogram. If these are confirmed outliers and removed a re-plotting of these histograms would provide a better view of the distribution of the data. We also see some skews in the data bieng confirmed. Price and Previous Owners are clearly right skewed distributions while Registration Year, Seats, and Engine may be approximately normal. More analysis would be needed to confirm this assumption

### Analysis of Bivariate Plots of Price:

```{r, echo = FALSE}
columns_to_plot = names(data)[!(names(data) %in% c("Price", "Body_Type"))]
pairs(data[, columns_to_plot], main = "Bivariate plots: Price vs others")
```

--- Comments about the above Bivariate plots ---

## Initial Preview of the Full Regression Model

Here we create a full model using all continuous predictors of "Price", the summary of which can be seen below:

```{r, echo = FALSE}
full_model = lm(Price~ . - Body_Type, data)
summary(full_model)
```

The estimate of the intercepts for each of the predictors makes intuituve sense.

### Mileage

The intercept of 0.02437 means that, on average and with all other variables held constant, an increase in mileage of one mile lowers the sale price by 0.02437 dollars. This makes intuitive sense, as a car with greater mileage is expected to have lesser value. The significance of the intercept is significant.

### Registration Year

The intercept of 715.4 means that, on average and with all other variables held constant, an increase in registration year by one year raises the sale price by 715.4 dollars. This makese sense, as newer cars are expected to be more valuable. The significance of the intercept is significant.

### Previous Owners

The intercept of -308.0 means that, on average and with all other variables held constant, an increase in previous owners by one owner lowers the sale price by 308 dollars. This makes sense, as cars that have been sold and resold more often have less value. The significance of the intercept is significant.

### Engine

The intercept of 2766 means that, on average and with all other variables held constant, an increase in engine size by 1 liter raises the sale price by 2766 dollars. This isn't as intuituve as the other predictors, but a larger, more powerful engines are expected to add value to the car. The significance of the intercept is significant.

### Doors

The intercept of -113.6 means that, on average and with all other variables held constant, an increase in the number of doors by 1 door lowers the sale price of the car by 113 dollars. This is not an intuitive prediction, and when looking at the p-value of this intercept, it is relatively large compared to the other predictors. Therefore, it might be wise to consider eliminating this variable from our final model.

### Seats

The intercept of -391.5 means that, on average and with all other variables held constant, an increase in the number of seats in the car by 1 lowers the sale price of the car by 391.5 dollars. This is also not as inuitive as the previous predictors, but the significance p-value seems to suggest that the number of seats could be a relevant predictor in our model. The significance of the intercept is significant.

### Initial GOF

Our initial GOF of 0.70 for the full model is moderately high, but we would like it to be larger. As of now, about 70% of the variability in the sale price of the cars is accounted for by our model.

# Model Diagnostics

## Checking Constant Variance Assumption

```{r, echo = FALSE}

par(mfrow = c(1, 2))

fitted_values = fitted(full_model)
residuals = resid(full_model)
Resid_Plot <- plot(fitted_values, residuals, main = "Residuals vs Fitted Values",
xlab = "Fitted Values", ylab = "Residuals")
abline(h=0)

Rstudent_residuals = rstudent(full_model)
RstudentResid_Plot <- plot(fitted_values, Rstudent_residuals, main = "R-student Residuals vs Fitted Values",
xlab = "Fitted Values", 
ylab = "R-student Residuals")
abline(h=0)
```

Reviewing both the plots of Residuals vs Fitted Values and the R-Student Residuals vs Fitted Values we see significant clustering around (7500,0) and there is a clear shape to the scattered points in the shape of a sort of crescent. A model with constant variance would have Residuals vs Fitted Value plots what are randomly scattered both above and below the baseline (y=0) and evenly spread across the extent of the fitted values. This is not what we see here, so from these graphs we would anecdotally expect that this model is heteroscedastic, but this may also indicate non linearity.

## Checking Normality using QQ-Plots and Tests

```{r, echo = FALSE}
qqnorm(residuals) #Residuals Already Defined in the evnironment above
qqline(residuals)
```

Points which closely follow the line would suggest residuals which are approximately normally distributed. In the qq plot above we see that this is somewhat true. The VAST majority of the residuals are normally distributed except for some at the top of the distribution around the third quantile. This is likely because these are outline points which don't align well to the model and therefore have very high residual values. A more quantitative test or revisiting this later would be helpful to clarify it.

```{r, echo = FALSE}
shapiro.test(residuals)
```

This is the Shapiro-Wilk normality test. For normality we would be looking for the p-value to be high, however this p-value of 2.2e-16 is just about as low as it gets. This would mean that the residuals (at least as they are now including the outliers) fail the normality test.
