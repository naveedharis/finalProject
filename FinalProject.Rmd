---
title: "Predictive Modeling of Used Car Prices with Statistical Linear Models"
subtitle: "Mth 439 Final Project"
author: "Haris Naveed, Griffin Lovato, Andrew Nahlik, Christian Calian, Jack Weitzner"
date: "2023-11-19"
output: pdf_document
---

## Data Cleanup:

First, we have to load in our data. The head of the data can be seen below: Be sure to download the file to the correct location on your machine or the connection will not open properly...

```{r, echo=FALSE}
raw_data = read.csv("used_cars_UK.csv", fileEncoding = "UTF-8")
head(raw_data)
```

Above, we can see what our raw data table looks like. The first step in cleaning this data is to encode the "Engine" column, currently considered a char, into a numeric variable. We will later revisit whether this categorical to quantitative encoding is valid. As we are focusing on continuous variables for this project, we will remove columns: "title", "Fuel.type", "Gearbox", "Emission Class", and "Service.history". The categorical variable "Body.type" will remain in our data set as a useful variable for stratification. We can also rename the columns to have a standardized form. Finally, we can remove any rows with NA values.

```{r, echo=FALSE}
data = raw_data[, c("Price", "Mileage.miles.", "Registration_Year", "Previous.Owners", "Body.type", "Engine", "Doors", "Seats")]
data$Engine = as.numeric(gsub("L", "", raw_data$Engine))
colnames(data)[colnames(data) == "Mileage.miles."] = "Mileage"
colnames(data)[colnames(data) == "Previous.Owners"] = "Previous_Owners"
colnames(data)[colnames(data) == "Body.type"] = "Body_Type"
data = na.omit(data)
head(data)
```

## Initial Data Analysis:

### Quick Summary:

A quick way to investigate the properties of our data set is to use the summary function:

```{r, echo=FALSE}
summary(data)
```

--- Comments about above summary ---

Based on the summary we can establish a few baseline assumptions about the data.

#### Price:

Beginning with price, the spread of the quartiles suggests that most of the vehicles are below \$6000 with the 3rd quartile bieng at \$8491. The maximum value being \$33900 suggests however that we are likely to have outliers in terms of price. This could be explained by luxury or collectible vintage vehicles.

#### Mileage:

The mileage seems like a relatively normal distriution. The median and mean values are approximately 81,000 with the first quartile being approximately 60,000 and the 3rd quartile being around 105000. These are both slightly more than 20,000 from the mean/median which would suggest a good distribution. The minimum value in mileage does give us a hint on price. A minimum value of mileage being 6 suggests that in this dataset there are essentially new cars. In considering building a model these "nearly new" should probably be removed from the model.

#### Registration Year:

Registration year is likely to be biased toward more modern years (right skewed distribution). We see a minimum datapoint in 1987, but 75% of the datapoints are for vehicles in the last 15 years (2009 or newer).

#### Previous Owners:

This is a long tailed distribution. From a practical knowledge perspective it is suprising to see even one vehicle has had 9 previous owners and is now on sale for a tenth time. Either this is a really old vehicle with a lot of miles or there is something wrong with it that makes it undesirable. It is also possible that this is an error in the data.

#### Body Type:

This is a categorical variable which the summarry() function cannot properly view. During later processing we can view this as a histogram and better understand what different categories these vehicles fall into.

#### Engine:

This is the engine displacement (eg. 2L engine). It is curious that the median and mean value for this parameter are 1.6L as this strikes us as somewhat low. That said, this dataset is for cars registered for sale in the UK which often has smaller and somewhat less powerful vehicles compared to the US. The maximum value is likely to be an outlier as a sportscar or something similar.

#### Doors:

This variable is likely somewhat difficult to categorize. A large amount of the vehicles (at least 50%) claim to have 5 doors, however this does not seem possible logically unless the trunk is bieng counted as a 'door'. If this is the case 5 door vehicles should likely be relabeled as 4 door vehicles. Most vehicles are likely to have a trunk and it intutively makes sense to have an even number of doors per vehicle.

#### Seats:

The distribution of the quartiles of seats suggests that the data is very short tailed and clustered around the 5 seats that we would expect in a standard sedan or SUV. Viewing the histogram of this variable should be able to help significantly. More analysis to come in the next section.

### Analysis of Histograms of Variables:

Additionally, we can investigate the histograms of each of our variables:

```{r, echo=FALSE}
par(mfrow = c(2, 2)) 
for (col in names(data)) {
  if (is.numeric(data[[col]])) {
    hist(data[[col]], main = paste("Histogram of", col), xlab= col)
  }
}
```

--- Comments about above histograms---

Viewing the Histograms helps to clarify some of the assumptions made in the summary phase. While a variable by variable break down is likely repetitious covering a few of the histograms is helpful. Price, Mileage, Registration Year, and engine all have suspected outlier values which distort the histogram. If these are confirmed outliers and removed a re-plotting of these histograms would provide a better view of the distribution of the data. We also see some skews in the data bieng confirmed. Price and Previous Owners are clearly right skewed distributions while Registration Year, Seats, and Engine may be approximately normal. More analysis would be needed to confirm this assumption

### Analysis of Bivariate Plots of Price:

```{r, echo = FALSE}
columns_to_plot = names(data)[!(names(data) %in% c("Price", "Body_Type"))]
pairs(data[, columns_to_plot], main = "Bivariate plots: Price vs others")
```

--- Comments about the above Bivariate plots ---

## Initial Preview of the Full Regression Model

Here we create a full model using all continuous predictors of "Price", the summary of which can be seen below:

```{r, echo = FALSE}
full_model = lm(Price~ . - Body_Type, data)
summary(full_model)
```

The estimate of the intercepts for each of the predictors makes intuituve sense.

### Mileage

The intercept of 0.02437 means that, on average and with all other variables held constant, an increase in mileage of one mile lowers the sale price by 0.02437 dollars. This makes intuitive sense, as a car with greater mileage is expected to have lesser value. The significance of the intercept is significant.

### Registration Year

The intercept of 715.4 means that, on average and with all other variables held constant, an increase in registration year by one year raises the sale price by 715.4 dollars. This makese sense, as newer cars are expected to be more valuable. The significance of the intercept is significant.

### Previous Owners

The intercept of -308.0 means that, on average and with all other variables held constant, an increase in previous owners by one owner lowers the sale price by 308 dollars. This makes sense, as cars that have been sold and resold more often have less value. The significance of the intercept is significant.

### Engine

The intercept of 2766 means that, on average and with all other variables held constant, an increase in engine size by 1 liter raises the sale price by 2766 dollars. This isn't as intuituve as the other predictors, but a larger, more powerful engines are expected to add value to the car. The significance of the intercept is significant.

### Doors

The intercept of -113.6 means that, on average and with all other variables held constant, an increase in the number of doors by 1 door lowers the sale price of the car by 113 dollars. This is not an intuitive prediction, and when looking at the p-value of this intercept, it is relatively large compared to the other predictors. Therefore, it might be wise to consider eliminating this variable from our final model.

### Seats

The intercept of -391.5 means that, on average and with all other variables held constant, an increase in the number of seats in the car by 1 lowers the sale price of the car by 391.5 dollars. This is also not as inuitive as the previous predictors, but the significance p-value seems to suggest that the number of seats could be a relevant predictor in our model. The significance of the intercept is significant.

### Initial GOF

Our initial GOF of 0.70 for the full model is moderately high, but we would like it to be larger. As of now, about 70% of the variability in the sale price of the cars is accounted for by our model.

# Model Diagnostics

## Checking Constant Variance Assumption

```{r, echo = FALSE}

par(mfrow = c(1, 2))

fitted_values = fitted(full_model)
residuals = resid(full_model)
Resid_Plot <- plot(fitted_values, residuals, main = "Residuals vs Fitted Values",
xlab = "Fitted Values", ylab = "Residuals")
abline(h=0)

Rstudent_residuals = rstudent(full_model)
RstudentResid_Plot <- plot(fitted_values, Rstudent_residuals, main = "R-student Residuals vs Fitted Values",
xlab = "Fitted Values", 
ylab = "R-student Residuals")
abline(h=0)
```

Reviewing both the plots of Residuals vs Fitted Values and the R-Student Residuals vs Fitted Values we see significant clustering around (7500,0) and there is a clear shape to the scattered points in the shape of a sort of crescent. A model with constant variance would have Residuals vs Fitted Value plots what are randomly scattered both above and below the baseline (y=0) and evenly spread across the extent of the fitted values. This is not what we see here, so from these graphs we would anecdotally expect that this model is heteroscedastic, but this may also indicate non linearity.

## Checking Normality using QQ-Plots and Tests

```{r, echo = FALSE}
qqnorm(residuals) #Residuals Already Defined in the evnironment above
qqline(residuals)
```

Points which closely follow the line would suggest residuals which are approximately normally distributed. In the qq plot above we see that this is somewhat true. The VAST majority of the residuals are normally distributed except for some at the top of the distribution around the third quantile. This is likely because these are outline points which don't align well to the model and therefore have very high residual values. A more quantitative test or revisiting this later would be helpful to clarify it.

```{r, echo = FALSE}
shapiro.test(residuals)
```

This is the Shapiro-Wilk normality test. For normality we would be looking for the p-value to be high, however this p-value of 2.2e-16 is just about as low as it gets. This would mean that the residuals (at least as they are now including the outliers) fail the normality test.

## Checking for High Leverage Points

We can check the leverage using the hatvalues function.

```{r, echo = FALSE}
leverage_values = hatvalues(full_model)
plot(leverage_values, main = "Leverage Values", xlab = "Observation", ylab = "Leverage")

```

Immediately upon finding and plotting the leverage values we can see that there are a few high leverage points. One point is EXTREMELY high leverage. In order to better work with these points lets find the points specifically that we would consider high leverage. In this case take "high leverage" to mean at least twice the mean leverage of all points.

```{r, echo = FALSE}
high_leverage_points <- which(leverage_values > 2 * mean(leverage_values))
length(high_leverage_points)
```

With the points isolated we see that there are actually 185 points that are high leverage which is many more than the initial estimate you might have just by viewing the graph. These points are now saved to the vector `high_leverage_points` for easy access later. Plotting the high leverage points in a different color is helpful to see which they are as well:

```{r, echo = FALSE}
plot(leverage_values, main = "Leverage Values", xlab = "Observation", ylab = "Leverage")
points(high_leverage_points, leverage_values[high_leverage_points], col = "red", pch = 16)

```

The car corresponding to index 1036 has the largest leverage point of 0.337. Upon inspecting the dataframe this car has over 1.1 million miles, so it makes sense why it has such great influence. There are many other notable points looking at this plot which have leverages over the threshold of 2\*(k+1/n), so we should consider eliminating those in the future.

## Checking for Outlier Points in y

To check for outlier's in y we can use the residuals vs fitted value plot, but since we already used this plot above lets use jackknife residuals of our model, and test that for H_0: No outliers.

```{r, echo = FALSE}
jackknife_residuals <- rstudent(full_model)
jackknife_residuals[which.max(abs(jackknife_residuals))]
qt(.05/(2248*2), 2240)
```

The largest residual identified had a value of 13.2 which is way larger than our critical value of 4.25, so we reject the null hypothesis that none of our responses are outlying, and conclude that at least one of the used car sale prices is an outlier in the dataset. Since it is so high above the critical value it is likely that we have more than 1 outlier but that is not something we would be able to tell from here.

```{r, echo = FALSE}
plot(fitted_values, jackknife_residuals, main = "Jackknife Residuals vs Fitted Values", 
     xlab = "Fitted Values", ylab = "Jackknife Residuals")

abline(h = 0, col = "red", lty = 2)

text(fitted_values[abs(jackknife_residuals) > 4.25], 
     jackknife_residuals[abs(jackknife_residuals) > 4.25], 
     labels = which(abs(jackknife_residuals) > 4.25), col = "blue")
legend("topright", legend = c("Outliers"), col = c("blue"), pch = 1)
```

This graph helps us identify points likely to be outliers. They are compared on a graph of the jackknife residuals vs fitted values and the points which are likely to be outliers based on this are marked in blue. We will remove these in the next part and assess again.

## Checking for Influential Points

We will use Cook's Distance to check for influential points, as it determines how much the fitted values change when the ith data point is removed. Cook's Distance values larger than 1 will be considered influential, and values lesser than 0.5 will be considered not influential.

```{r, echo = FALSE}
cook_full <- cooks.distance(full_model)
plot(cook_full, ylab="Cook's Distance", main="Cook's Distance", type="l")
cook_full[which.max(cook_full)]
```

Observation 1036 is determined to be a very obvious outlier. This same index was previously determined to have very high leverage. Given these factors, it would be beneficial to remove this point from our dataset.

I will also renumber the indices in our dataset, because observation 1036 corresponds to index 620 following data cleanup. This will align the two values.

### Renumbering indices

```{r, echo = FALSE}
new_indices <- 1:nrow(data) 
rownames(data) <- new_indices
```

### Removing observation 620 (formerly observation 1036)

```{r, echo = FALSE}
data_2 <- data[-c(620, 706, 718, 1158, 1958, 1764, 1967, 1893), ]
```

### Reanalyzing fit of model and Cook's Distance without this point

```{r, echo = FALSE}
full_model_2 <- lm(Price~ . - Body_Type, data_2)
summary(full_model_2)
```

We see the adjusted R squared changed by about 0.0437 after removing these points.

```{r, echo = FALSE}
cook_full_2 <- cooks.distance(full_model_2) 
plot(cook_full_2, ylab="Cook's Distance", main="Cook's Distance", type="l")
cook_full_2[which.max(cook_full_2)]
```

Our new largest Cook's Distance is 0.05, which is much less than our threshold of 0.5. This tells us that our new dataset does not contain significantly influential points.

## Checking for Outlier Points in y (attempt 2)

We will look at the jackknife residuals of our new full model, and test againat the null hypothesis that none of our responses are outliers.

```{r, echo = FALSE}
new_jackknife_residuals <- rstudent(full_model_2)
new_jackknife_residuals[which.max(abs(new_jackknife_residuals))]
qt(.05/(2240*2), 2232)
```

Our largest jackknife residual of 5.135 is larger than our critical value of 4.25, so we reject the null hypothesis that none of our responses are outlying, and conclude that at least one of the used car sale prices is an outlier in the dataset.

```{r, echo = FALSE}
new_fitted_values = fitted(full_model_2)
plot(new_fitted_values, new_jackknife_residuals, main = "Jackknife Residuals vs Fitted Values", 
     xlab = "Fitted Values", ylab = "Jackknife Residuals")

abline(h = 0, col = "red", lty = 2)

text(new_fitted_values[abs(new_jackknife_residuals) > 4.25], 
     new_jackknife_residuals[abs(new_jackknife_residuals) > 4.25], 
     labels = which(abs(new_jackknife_residuals) > 4.25), col = "blue")
legend("topright", legend = c("Outliers"), col = c("blue"), pch = 1)
```

We have now identified an additional point, 1184 as an outlier. Lets drop that and check again.

```{r, echo = FALSE}
data_2 <- data_2[-c(1184), ]
full_model_2 <- lm(Price~ . - Body_Type, data_2)
new_jackknife_residuals <- rstudent(full_model_2)
new_jackknife_residuals[which.max(abs(new_jackknife_residuals))]
qt(.05/(2240*2), 2232)
```

We can see here that the jackknife residuals are approaching the critical value. It isnt quite under the critical value but it is remarkably close.

## Checking the structure of the model

```{r, echo = FALSE}
plot(full_model_2)
```

Having already dropped the points that were identified as outliers above all remaning points are well within the cook distance even if they do stray from the line.

## Exploring Possibility of a Box-Cox Transformation

```{r, echo = FALSE}
library(MASS)
boxcox(full_model_2, plotit=T)
boxcox(full_model_2, plotit=T, lambda=seq(0.1, 0.3, by=0.05))
```

As we can see in our plot, lambda = 1 is not within the range of plausible values. As a result, it would be good to perform a box-cox transformation on our model. Specifically, the center of our interval lies around 0.16 = (1/6), so a transformation of the response to the power of 1/6 seems plausible.

## Creating new model with transformation

```{r, echo = FALSE}
full_model_transformed <- lm(Price^(1/6)~ . - Body_Type, data_2)
summary(full_model_transformed)
```

Looking at this summary, we can see that our adjusted R-squared value increased significantly from .72 to 0.8. The significance of our predictors remained largely unchanged; however, the magnitudes of the betas are drastically different.

## Reperforming Model Diagnostics

### Checking for homoskedasticity

```{r, echo = FALSE}
plot(full_model_transformed)
```

When we revisit the plot of residuals vs fitted values, the previously seen crescent shape has turned into a more evenly scattered distribution, which is what we wanted to see. Observations 706 and 718 still have very large residuals. Overall, the distribution looks more homoskedastic.

### Checking normality assumption using qq-plots and S-W Normality Test

Looking at the QQ-Residuals plot, the vast majority of the residuals fall on the qq-line. However, a few in partifular stray far from this line, notable 1531.

```{r, echo = FALSE}
shapiro.test(resid(full_model_transformed))
```

When we reperform the S-W test, we once again receive an extremely low p-value, indicating that we should conclude that our data is still not normally distributed.

# 5

## Exploring Serial Correlation in the Errors

```{r, echo=TRUE}
install.packages("lmtest")
library(lmtest)


serialModel = lm(Price~ . - Body_Type, data=data_2)
dwtest(serialModel)
```

Serial correlation is the relationship between a given variable and a lagged version of itself over various time intervals. It measures the relationship between a variable's current value given its past values. A variable that is serially correlated indicates that it may not be random.
DW statistic is quite close to 2, suggesting minimal autocorrelation, the significant p-value indicates there is some evidence of positive autocorrelation in the model's residuals. This could mean that consecutive residuals are not completely independent of each other.

## Applying a generalized least squares estimator

```{r, echo = FALSE}
library(nlme)
generalModel <- gls(Price~ . - Body_Type, correlation=corARMA(p=1), data=data_2)
summary(generalModel)
```

We see that the estimated value of rho is 0.07. To check if this is significant, we then compute its confidence interval:

# 6

## Explore the possibility that the variance of the errors depend linearly on a specific predictor variable.

For this exploration, the predictor variable will be mileage

```{r, echo=TRUE}
install.packages('ggplot2')
library(ggplot2)
model <- lm(Price ~ Mileage, data=data_2)
residuals <- resid(model)
ggplot(data_2, aes(x = Mileage, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red")
```

The spread of residuals indicates that the linear model might not be capturing all the variability in the data, particularly for cars with higher mileage. This could suggest that a simple linear model may not be the best fit for this data and that a more complex model or transformation of variables might be needed. Additionally, the plot suggests that further investigation into the causes of this pattern is warranted, which might include considering non-linear effects, interaction effects, or other forms of variance-stabilizing transformations. Weighted least squares or other forms of modeling that account for the heteroscedastic nature of the data might be more appropriate here. In the presence of heteroscedasticity, the standard errors of the coefficient estimates can be inefficient, and hypothesis tests (like the t-test for coefficients) can be unreliable.

```{r, echo=TRUE}
bin_width <- 10000
max_mileage <- max(data_2$Mileage)
bins <- seq(0, max_mileage, by = bin_width)
binned_data <- cut(data_2$Mileage, breaks = bins, include.lowest = TRUE, right = FALSE)
variance_data <- tapply(residuals, binned_data, var)

# Create the plot
bin_centers <- bins[-length(bins)] + bin_width / 2
df_plot <- data.frame(Mileage = bin_centers, Variance = variance_data)
ggplot(df_plot, aes(x = Mileage, y = Variance)) +
  geom_line() +
  xlab("Mileage") +
  ylab("Variance of Residuals") +
  ggtitle("Variance of Residuals vs Mileage") + 
  scale_x_continuous(labels = scales::comma) + # Format x-axis labels
  scale_y_continuous(labels = scales::comma) 
```

The plot has peaks and valleys, indicating that the variance of residuals is not constant across different mileage values. In some mileage ranges, the variance is higher, whereas it's lower in others. Aside from the sharp peak, there seems to be a general decreasing trend in variance with increasing mileage up to a certain point, after which it levels off and possibly starts increasing again. This could imply that the relationship between car price and mileage is not uniform across all mileage levels and that the variance in pricing becomes more consistent in the middle mileage range before possibly increasing again.The non-constant variance across mileage values is indicative of heteroscedasticity. Ideally, for homoscedasticity, we would expect to see a more or less flat line, indicating that the variance of residuals is the same across all levels of mileage.

## Apply weighted least-squares estimation with predictor variable

```{r, echo=TRUE}
plot(data_2$Mileage, data_2$Price, main="Price vs. Mileage",
     xlab="Mileage", ylab="Price", pch=19)

abline(model, col="red")

```

```{r, echo=TRUE}
par(mfrow=c(2,3))
plot(model, which=1:6)
```
Residuals vs Fitted Values: It is biased and heteroscedastic. 

Q-Q Residuals: The points generally follow the line of identity but with some deviations, especially in the tails. This suggests that the residuals are approximately normal but with potential outliers or heavy tails, indicating more extreme values than would be expected for a normal distribution. This could mean that the underlying assumptions of normality for statistical tests or models might not be fully met.


```{r, echo=TRUE}
# iteration 0
weight0 = rep(1, length(data_2$Mileage))
model.w1 = lm(Price ~ Mileage, data=data_2)

# iteration 1
resid.w1 = model.w1$resid # OLS
sd.w1 = lm(abs(resid.w1) ~ Mileage, data = data_2) # IRLS
weight1 = (1/sd.w1$fitted)^2
model.w2 = lm(Price ~ Mileage, data = data_2, weights=weight1) # WLS

# iteration 2
resid.w2 = model.w2$resid
sd.w2 = lm(abs(resid.w2) ~ Mileage, data = data_2)
weight2 = (1/sd.w2$fitted)^2
model.w3 = lm(Price ~ Mileage, data = data_2, weights=weight2)

```


```{r, echo=TRUE}
par(mfrow=c(1,2))

# Plot for Iteration 1
plot(data_2$Mileage, data_2$Price, main="Iteration 1", xlab="Mileage", ylab="Price", pch=19, col='grey')
abline(model.w1, col='green')
abline(sd.w1, col='blue')
abline(model.w2, col="red") 

legend("topright",            
       legend = c("True Line", "IRLS 1", "WLS"),  # Legend labels
       col = c("green", "blue", "red"),           # Colors
       lty = c(1, 1, 1),                          # Line types
       cex = 0.8)

# Plot for Iteration 2
plot(data_2$Mileage, data_2$Price, main="Iteration 2", xlab="Mileage", ylab="Price", pch=19, col='grey')
abline(model.w1, col='green')
abline(sd.w2, col='blue')
abline(model.w2, col="red")
abline(model.w3, col="purple")
legend("topright",            
       legend = c("True Line", "IRLS 1", "WLS", "IRLS 2"),  # Legend labels
       col = c("green", "blue", "red", "purple"),           # Colors
       lty = c(1, 1, 1, 1),                          # Line types
       cex = 0.8)

```
The IRLS 1 line is quite close to the true line, although it appears to underestimate the price at lower mileage and overestimate it at higher mileage compared to the true line. Since this is only the first iteration, further iterations could potentially bring the IRLS line closer to the true line. The WLS line deviates more from the true line than the IRLS line, especially at the higher mileage end of the plot. This could suggest that the initial weights assigned in the WLS method are not optimal or that the model does not account for all the variabilities in the data.

Comparing the IRLS 1 and IRLS 2 lines, we can evaluate how the IRLS algorithm is adjusting with each iteration. Typically, with more iterations, the IRLS method would refine its estimates, getting closer to the true line as it minimizes the influence of outliers or leverages the structure in the data.

```{r, echo=TRUE}
par(mfrow=c(2,2))
plot(fitted(model.w2), resid(model.w2))
plot(sqrt(weight1) * fitted(model.w2), sqrt(weight1) * resid(model.w2))

plot(fitted(model.w3), resid(model.w3))
plot(sqrt(weight2) * fitted(model.w3), sqrt(weight2) * resid(model.w3))

```
The first plot of model.w2 shows a clear pattern where residuals fan out as the fitted values increase. This suggests the presence of heteroscedasticity, meaning that the variance of the residuals is not constant across all levels of the fitted values. There are some large residuals (both positive and negative) especially at the higher end of the fitted values. This indicates that the model may not be capturing all the explanatory information, and there may be outliers or leverage points that the model isn’t handling well.

The second plot of model.w2 shows that the data points are clustered near the vertical axis, indicating most fitted values are low when weighted by the square root of the weights in the first iteration. There is a single outlier far to the right, suggesting there is at least one observation with a much higher fitted value when adjusted by its weight.

In the first plot of model.w3, there seems to be a funnel shape, with the residuals spreading out as the fitted values increase, which suggests heteroscedasticity. The funnel shape in this plot indicates that the model might benefit from a transformation of the response variable or the use of a different modeling approach that accounts for the changing variance.

In the second plot of model.w3, most data points are clustered around the lower end of the transformed fitted values, with a few points standing out. This could suggest that the model fits well for a range of the data but may not capture some points effectively.


# 7. Assess potential multicollinearity as in Topic 16

In order to assess multicollinearity between the predictors, we must create a list of the eigenvalues. First, we establish the design matrix of the model, and remove the first column (the intercept). Then, we call eigen(Xstar'\*Xstar) to get a list of the eigenvalues of the matrix.

```{r}
design_matrix <- model.matrix(full_model)
design_matrix_star <- design_matrix[,-1]
e<-eigen(t(design_matrix_star)%*%design_matrix_star)
```

With e now representing a list of the eigenvalues, we can print them out to see any trends

```{r}
e$val
```

We can see that the eigenvalues have large differences in scale. To actually determine of we have a collinearirty problem, we must determine the condition number of each one.

```{r}
sqrt(e$val[1]/e$val)
```

This has revealed that the condition number of every eigenvalue is larger than 30, ranging from \~101 to \~220929. This strongly suggests that our predictors have a high degree of collinearity, leading to problems arising from the columns of the design matrix being linear combinations of each other.

```{r}
library(faraway)
```

Now, let's check the variance inflation factor (VIF) of each of the predictors. We must utilize the faraway library to access the vif() function.

```{r}
vif(design_matrix_star)
```

Calling vif() on the design matrix (without the first column) indicates the variance inflation factor for each predictor, of the factor by which the variance of that regressor is increased due to dependence on the other regressors. As all the VIFs are below 10, this suggests that the variances of each regressor are not impacted by that regressors's dependence on the other regressors.

# 8. If possible, apply an exhaustive model selection search as in Topic 17 using the different selection criteria we covered in class

We can use the leaps library to access the leaps() and regsubsets() functions, which we will need to apply an exhaustive model selection search.

```{r}
install.packages('leaps')
library(leaps)
```

We will make two new variables representing the inputs (leaps_x) and output (leaps_y) of our data from our original dataset.

```{r}
leaps_x <- data[, -1]
leaps_y <- data$Price
```

Next, we will perform extra cleaning on the data using the complete.cases() function, which removes rows that have NAs, which the leaps() function is particuarly sensitive to.

```{r}
data_clean<-complete.cases(leaps_x, leaps_y)
leaps_x_clean<-leaps_x[data_clean,]
leaps_y_clean<-leaps_y[data_clean]
```

Under normal circumstances, the line below, that calls for leaps() on our data to perform all possible regression methodologies, would work as intended. Problematically, if data has high collinearity (as seen in part 7), leaps() may not function properly or even crash RStudio.

```{r}
#leaps_adjr2_result<-leaps(leaps_x_clean, leaps_y_clean, method="adjr2")
#Running the above line with any method ("r2", "adjr2", or "Cp") crashes RStudio. Searching indicated that this is a known issue with leaps() if the data has very high collinearity, which it does as indicated in Task 7.
```

Fortunately, there are other methods that we can use to apply an exhaustive model selection search to our data. This comes in the form of the leaps library's regsubsets() function. We will use it to determine a ranking of the models based on the adjusted r-squared value.

```{r}
subsets_results_1 <- regsubsets(Price~Mileage+Registration_Year+Previous_Owners+Engine+Doors+Seats, data=data, nbest=1, method="exhaustive")
```

Here, we establish subsets_results_1 as a result of regsubsets() on our data, adding the best parameter 1 at a time until we are using all 6 parameter for a total of 6 models. Note that we have removed the Body_Type parameter from the input of our regsubsets(), as the function splits up catagorical variables so that each response becomes its own parameter with a value of "1" or "0", as regsubsets() is built for numerical parameters.

```{r}
regsubsets_1 <- summary(subsets_results_1)
regsubsets_1
```

A summary of the results shows that Registration_Year is the best parameter to use if we can only use 1 parameter, followed (in order) by Engine, Mileage, Previous_Owners, Seats, and finally Doors. We can observe the adjusted r-squared visually by plotting it.

```{r}
par(mfrow=c(1,2))
plot(2:7, regsubsets_1$adjr2, xlab="No. of Parameters", ylab="Adjusted R-square")
plot(subsets_results_1, scale="adjr2")
```

Starting with the left plot, we can see that the adjusted r-squared increases the more parameters we have (assuming we always have the intercept, which is why it starts at 2), although it flattens out once we have 4 parameters at around 0.695. The right plot indicates which parameters we choose at each adjusted r-squared value, providing the same information as the summary pf regsubsets_1 above.

What if we want to know which models are best if we can examine the best 2 models instead of the single best model for each \# of parameters? We can check by adjusting the nbest value in regsubsets() from 1 to 2.

```{r}
subsets_results_2<-regsubsets(Price~Mileage+Registration_Year+Previous_Owners+Engine+Doors+Seats, data=data, nbest=2, method="exhaustive")
```

Here, we establish subsets_results_2 as a result of regsubsets() on our data, checking the top 2 parameter additions each time until we are using all 6 parameters for a new total of 11 models. We will keep Body_Type removed for the same reasons as in subsets_results_1.

```{r}
regsubsets_2<-summary(subsets_results_2)
regsubsets_2
```

A summary of subsets_results_2 shows that, again, Registration_Year is the best parameter to work with if we only have access to 1 parameter. However, using only Mileage creates a higher adjusted r-squared model than using only Engine, even though Engine was the 2nd parameter added when nbest=1 the last time we ran regsubsets().

```{r}
par(mfrow=c(1,2))
plot(c(2,2,3,3,4,4,5,5,6,6,7), regsubsets_2$adjr2, xlab="No. of Parameters", ylab="Adjusted R-square")
plot(subsets_results_2, scale="adjr2")
```

Plotting out the results of regsubsets_2 and subsets_results_2 shows similar plots to the first, as the adjusted r-squared value generally goes up with the number of parameters. The duplicate values at the same number of parameters indicate that we are showing the best 2 different combinations of parameters at that \# of parameters. Like the last plots, we can see that the adjusted r-squared value goes up as we add additional parameters, reaching its maximum of \~0.70 once we have added all 6 parameters (not including the intercept). This suggests that, despite the high collinearity revealed from our analysis in part 7, the best combination of (numerical) parameters, in terms of achieving the lowest r-squared value, is all of them.

# 9. Apply backward and forward model selections as described in Topic 18.

We can apply the built in function "step()" to apply forward selection to our full model. One should note that the step function utilizes AIC for selection criteria. As our full model after the box-cox transformation outperformed our standard full model, we will be applying forward selection to our transformed model. 

```{r, echo = FALSE}
forwad_selected_model = step(full_model_transformed, direction = 'forward')
summary(forwad_selected_model)
```

Applying forward selection to our transformed full model results in our model retaining the same predictors as the full model. As the step function utilizes a stopping criteria based on AIC, we retain predictors with large p-values such as the "Doors" predictor. To enforce our own stopping criteria based on p-values rather than AIC, we can utilize the update() function to manually add one feature at a time inside of a loop that only adds predictors to our model if their addition results in a model with all p-values < .05. The predictors will be added to the model in order of "best to worst", where this order has been determined earlier by regsubsets.

```{r, echo = FALSE}
forward_selected_model_pval = lm(Price^(1/6) ~ 1, data = data_2)
set_of_candidate_vars = c("Registration_Year", "Engine", "Mileage", "Previous_Owners", "Seats", "Doors")
while (TRUE) {
  best_p_value = 1
  for (candidate_var in set_of_candidate_vars) {
    temp_formula = update(formula(forward_selected_model_pval), paste(". ~ . +", candidate_var))
    temp_model = lm(temp_formula, data = data_2)
    existing_p_values = summary(temp_model)$coef[, "Pr(>|t|)"]
    max_p_value = max(existing_p_values[-1], na.rm = TRUE)
    if (max_p_value < best_p_value) {
      best_p_value = max_p_value
      add_variable = candidate_var
    }
  }
  if (best_p_value < 0.05) {
    forward_selected_model_pval = update(forward_selected_model_pval, paste(". ~ . +", add_variable))
    set_of_candidate_vars = set_of_candidate_vars[set_of_candidate_vars != add_variable]
  } else {
    break
  }
}
summary(forward_selected_model_pval)
```

This second attempt at forward selection based on p-values resulted in a model that removes the "Doors" predictor. This new model maintains the same R-squared value as the full model but with one less predictor, leading to a marginally improved adjusted R-squared value. As forward and backward selection don't necessarily result in the same feature selection, we can apply backward selection to see if the results vary. 

```{r, echo = FALSE}
backward_selected_model = step(full_model_transformed, direction = 'backward')
summary(backward_selected_model)
```

The backward selected model results in different feature selection when compared to the forward selected model with AIC stopping criteria, but does result in the same model as the forward selected model utilizing a p-value based stopping criteria. While a marginal improvement, these results suggest that the "Doors" predictor should be removed from our final model to avoid adding unnecessary complexity to our model. 

#10. Redo the model diagnostics in item 3 for one of the reduced model selected in item 9.

We will do the following analysis on the backward selected model.

## Checking constant variance assumption

We can utilize Residuals vs Fitted Values and the R-Student Residuals vs Fitted Values plots to check for homoscedasticity of our new model.

```{r, echo = FALSE}
par(mfrow = c(1, 2))
fitted_values = fitted(backward_selected_model)
residuals = resid(backward_selected_model)
Resid_Plot = plot(fitted_values, residuals, main = "Residuals vs Fitted Values", xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0)
Rstudent_residuals = rstudent(backward_selected_model)
RstudentResid_Plot = plot(fitted_values, Rstudent_residuals, main = "R-student Residuals vs Fitted Values", xlab = "Fitted Values", ylab = "R-student Residuals")
abline(h = 0)
```

Analyzing the above plots of Residuals vs Fitted Values and the R-Student Residuals vs Fitted Values, we can note a significant difference when compared to the same plots called in item 3) of this report. In the plots from item 3) there was a clear shape in the form of a crescent in the lower right quadrant of our plots, suggesting our model could be heteroscedastic or nonlinear. Our new plots are largely centered around y=0 and don't show any clear shapes or patterns. While the data seems to have a relatively even spread across fitted values, this somewhat breaks down at the largest fitted values, where there is a slightly smaller spread than the rest of the plot. This suggests that our homoscedasticity assumption is likely valid for the majority of our data set, but might break down for the most expensive cars. 

## Checking Normality using QQ-Plots and Test

We can utilize qq plots to analyze the validity of our normality assumption.

```{r, echo = FALSE}
qqnorm(residuals)
qqline(residuals)
```

Under our normality assumption we would expect a qq-plot of the residuals to follow a linear relation. While the majority of points in the middle of our qq-plot follow a linear relation, the overall "S" shape of our plot suggests our normality assumption may not be valid. The fact that that the left tail is particularly heavy suggests that our normality assumption is likely not valid for the lowest priced cars. For a more quantitative analysis, we can employ a Shapiro-Wilk normality test:

```{r, echo = FALSE}
shapiro.test(residuals)
```
Under normality we expect a large p-value, leading to the conclusion that our residuals fail the normality test. Given the heavy tail of our qq-plot, we might see better (more likely normal) results by removing the cheapest cars in our data set. 

## Checking for High Leverage Points

We can check the leverage using the hatvalues function.

```{r, echo = FALSE}
leverage_values = hatvalues(backward_selected_model)
plot(leverage_values, main = "Leverage Values", xlab = "Observation", ylab = "Leverage")
```

From inspection of the above plot we can determine there are a number of high leverage points. By considering points with more than twice the mean leverage as high leverage points, we can quantify exactly how many points are high leverage. 

```{r, echo = FALSE}
high_leverage_points = which(leverage_values > 2 * mean(leverage_values))
length(high_leverage_points)
```
This large number of high leverage points suggests a small portion of our data (~10%) may be highly influencing the overall behavior of the model. We can refit our working model with data excluding the high leverage points, but it is important to note that it might be dangerous to remove such a significant portion of our data set due to information loss and data integrity considerations.


```{r}
backward_selected_model_filtered_leverage  = lm(Price^(1/6) ~ . -Body_Type -Doors, data = data_2[-high_leverage_points, ])
summary(backward_selected_model_filtered_leverage)
```
This resulted in an improvement of our R-squared and adjusted R-squared values and interestingly reduced the determined statistical significance of the "Seats" predictor. 

## Checking for Outlier Points in Price

To check for outlier's in y we can use the residuals vs fitted value plot, but since we already used this plot above we can use jackknife residuals of our model to test H_0: No outliers. We will do this test first on our original backward selected model, and then on the backward selected model with the high leverage points filtered out. 

```{r, echo = FALSE}
jackknife_residuals <- rstudent(backward_selected_model)
jackknife_residuals[which.max(abs(jackknife_residuals))]
qt(.05/(2248*2), 2240)
jackknife_residuals_filtered <- rstudent(backward_selected_model_filtered_leverage)
jackknife_residuals_filtered[which.max(abs(jackknife_residuals_filtered))]
qt(.05/(2248*2), 2240)
```
The results of the jackknife residuals of the original backward selected model suggest that we should reject H0 and conclude that at least one value in our data set is an outlier. The maximum jackknife residual of the backward selected model with high leverage points removed is less than the critical value, leading to the conclusion that we fail to reject H0. This makes sense as high leverage points are often associated with outliers.

Returning to just our original backward selected model, we can visualize the outliers in the non-filtered data set via a plot of Jackknife Residuals vs Fitted Values:

```{r, echo = FALSE}
plot(fitted_values, jackknife_residuals, main = "Jackknife Residuals vs Fitted Values", 
     xlab = "Fitted Values", ylab = "Jackknife Residuals")
abline(h = 0, col = "red", lty = 2)
text(fitted_values[abs(jackknife_residuals) > 4.25], 
     jackknife_residuals[abs(jackknife_residuals) > 4.25], 
     labels = which(abs(jackknife_residuals) > 4.25), col = "blue")
legend("topright", legend = c("Outliers"), col = c("blue"), pch = 1)
```
Here we can identify an outlier in the point at index 1526, suggesting it might be worth removing from future models.

## Checking for Influential Points

We can again apply Cook's Distance to check for influential points, as it determines how much the fitted values change when the ith data point is removed. 

```{r, echo = FALSE}
cook_backward = cooks.distance(backward_selected_model)
plot(cook_backward, ylab="Cook's Distance", main="Cook's Distance", type="l")
cook_backward[which.max(cook_backward)]
```

We can identify the index of the maximum Cook's distance point as 1531, suggesting it might be worth removing from any future models.