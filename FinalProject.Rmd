---
title: "Predictive Modeling of Used Car Prices with Statistical Linear Models"
subtitle: "Mth 439 Final Project"
author: "Haris Naveed, Griffin Lovato, Andrew Nahlik, Christian Calian, Jack Weitzner"
date: "2023-11-19"
output: pdf_document
---

## Data Cleanup:

First, we have to load in our data. The head of the data can be seen below: Be sure to download the file to the correct location on your machine or the connection will not open properly...

```{r, echo=FALSE}
raw_data = read.csv("used_cars_UK.csv", fileEncoding = "UTF-8")
head(raw_data)
```

Above, we can see what our raw data table looks like. The first step in cleaning this data is to encode the "Engine" column, currently considered a char, into a numeric variable. We will later revisit whether this categorical to quantitative encoding is valid. As we are focusing on continuous variables for this project, we will remove columns: "title", "Fuel.type", "Gearbox", "Emission Class", and "Service.history". The categorical variable "Body.type" will remain in our data set as a useful variable for stratification. We can also rename the columns to have a standardized form. Finally, we can remove any rows with NA values.

```{r, echo=FALSE}
data = raw_data[, c("Price", "Mileage.miles.", "Registration_Year", "Previous.Owners", "Body.type", "Engine", "Doors", "Seats")]
data$Engine = as.numeric(gsub("L", "", raw_data$Engine))
colnames(data)[colnames(data) == "Mileage.miles."] = "Mileage"
colnames(data)[colnames(data) == "Previous.Owners"] = "Previous_Owners"
colnames(data)[colnames(data) == "Body.type"] = "Body_Type"
data = na.omit(data)
head(data)
```

## Initial Data Analysis:

### Quick Summary:

A quick way to investigate the properties of our data set is to use the summary function:

```{r, echo=FALSE}
summary(data)
```

--- Comments about above summary ---

Based on the summary we can establish a few baseline assumptions about the data.

#### Price:

Beginning with price, the spread of the quartiles suggests that most of the vehicles are below \$6000 with the 3rd quartile bieng at \$8491. The maximum value being \$33900 suggests however that we are likely to have outliers in terms of price. This could be explained by luxury or collectible vintage vehicles.

#### Mileage:

The mileage seems like a relatively normal distriution. The median and mean values are approximately 81,000 with the first quartile being approximately 60,000 and the 3rd quartile being around 105000. These are both slightly more than 20,000 from the mean/median which would suggest a good distribution. The minimum value in mileage does give us a hint on price. A minimum value of mileage being 6 suggests that in this dataset there are essentially new cars. In considering building a model these "nearly new" should probably be removed from the model.

#### Registration Year:

Registration year is likely to be biased toward more modern years (right skewed distribution). We see a minimum datapoint in 1987, but 75% of the datapoints are for vehicles in the last 15 years (2009 or newer).

#### Previous Owners:

This is a long tailed distribution. From a practical knowledge perspective it is suprising to see even one vehicle has had 9 previous owners and is now on sale for a tenth time. Either this is a really old vehicle with a lot of miles or there is something wrong with it that makes it undesirable. It is also possible that this is an error in the data.

#### Body Type:

This is a categorical variable which the summarry() function cannot properly view. During later processing we can view this as a histogram and better understand what different categories these vehicles fall into.

#### Engine:

This is the engine displacement (eg. 2L engine). It is curious that the median and mean value for this parameter are 1.6L as this strikes us as somewhat low. That said, this dataset is for cars registered for sale in the UK which often has smaller and somewhat less powerful vehicles compared to the US. The maximum value is likely to be an outlier as a sportscar or something similar.

#### Doors:

This variable is likely somewhat difficult to categorize. A large amount of the vehicles (at least 50%) claim to have 5 doors, however this does not seem possible logically unless the trunk is bieng counted as a 'door'. If this is the case 5 door vehicles should likely be relabeled as 4 door vehicles. Most vehicles are likely to have a trunk and it intutively makes sense to have an even number of doors per vehicle.

#### Seats:

The distribution of the quartiles of seats suggests that the data is very short tailed and clustered around the 5 seats that we would expect in a standard sedan or SUV. Viewing the histogram of this variable should be able to help significantly. More analysis to come in the next section.

### Analysis of Histograms of Variables:

Additionally, we can investigate the histograms of each of our variables:

```{r, echo=FALSE}
par(mfrow = c(2, 2)) 
for (col in names(data)) {
  if (is.numeric(data[[col]])) {
    hist(data[[col]], main = paste("Histogram of", col), xlab= col)
  }
}
```

--- Comments about above histograms---

Viewing the Histograms helps to clarify some of the assumptions made in the summary phase. While a variable by variable break down is likely repetitious covering a few of the histograms is helpful. Price, Mileage, Registration Year, and engine all have suspected outlier values which distort the histogram. If these are confirmed outliers and removed a re-plotting of these histograms would provide a better view of the distribution of the data. We also see some skews in the data bieng confirmed. Price and Previous Owners are clearly right skewed distributions while Registration Year, Seats, and Engine may be approximately normal. More analysis would be needed to confirm this assumption

### Analysis of Bivariate Plots of Price:

```{r, echo = FALSE}
columns_to_plot = names(data)[!(names(data) %in% c("Price", "Body_Type"))]
pairs(data[, columns_to_plot], main = "Bivariate plots: Price vs others")
```

--- Comments about the above Bivariate plots ---

## Initial Preview of the Full Regression Model

Here we create a full model using all continuous predictors of "Price", the summary of which can be seen below:

```{r, echo = FALSE}
full_model = lm(Price~ . - Body_Type, data)
summary(full_model)
```

The estimate of the intercepts for each of the predictors makes intuituve sense.

### Mileage

The intercept of 0.02437 means that, on average and with all other variables held constant, an increase in mileage of one mile lowers the sale price by 0.02437 dollars. This makes intuitive sense, as a car with greater mileage is expected to have lesser value. The significance of the intercept is significant.

### Registration Year

The intercept of 715.4 means that, on average and with all other variables held constant, an increase in registration year by one year raises the sale price by 715.4 dollars. This makese sense, as newer cars are expected to be more valuable. The significance of the intercept is significant.

### Previous Owners

The intercept of -308.0 means that, on average and with all other variables held constant, an increase in previous owners by one owner lowers the sale price by 308 dollars. This makes sense, as cars that have been sold and resold more often have less value. The significance of the intercept is significant.

### Engine

The intercept of 2766 means that, on average and with all other variables held constant, an increase in engine size by 1 liter raises the sale price by 2766 dollars. This isn't as intuituve as the other predictors, but a larger, more powerful engines are expected to add value to the car. The significance of the intercept is significant.

### Doors

The intercept of -113.6 means that, on average and with all other variables held constant, an increase in the number of doors by 1 door lowers the sale price of the car by 113 dollars. This is not an intuitive prediction, and when looking at the p-value of this intercept, it is relatively large compared to the other predictors. Therefore, it might be wise to consider eliminating this variable from our final model.

### Seats

The intercept of -391.5 means that, on average and with all other variables held constant, an increase in the number of seats in the car by 1 lowers the sale price of the car by 391.5 dollars. This is also not as inuitive as the previous predictors, but the significance p-value seems to suggest that the number of seats could be a relevant predictor in our model. The significance of the intercept is significant.

### Initial GOF

Our initial GOF of 0.70 for the full model is moderately high, but we would like it to be larger. As of now, about 70% of the variability in the sale price of the cars is accounted for by our model.

# Model Diagnostics

## Checking Constant Variance Assumption

```{r, echo = FALSE}

par(mfrow = c(1, 2))

fitted_values = fitted(full_model)
residuals = resid(full_model)
Resid_Plot <- plot(fitted_values, residuals, main = "Residuals vs Fitted Values",
xlab = "Fitted Values", ylab = "Residuals")
abline(h=0)

Rstudent_residuals = rstudent(full_model)
RstudentResid_Plot <- plot(fitted_values, Rstudent_residuals, main = "R-student Residuals vs Fitted Values",
xlab = "Fitted Values", 
ylab = "R-student Residuals")
abline(h=0)
```

Reviewing both the plots of Residuals vs Fitted Values and the R-Student Residuals vs Fitted Values we see significant clustering around (7500,0) and there is a clear shape to the scattered points in the shape of a sort of crescent. A model with constant variance would have Residuals vs Fitted Value plots what are randomly scattered both above and below the baseline (y=0) and evenly spread across the extent of the fitted values. This is not what we see here, so from these graphs we would anecdotally expect that this model is heteroscedastic, but this may also indicate non linearity.

## Checking Normality using QQ-Plots and Tests

```{r, echo = FALSE}
qqnorm(residuals) #Residuals Already Defined in the evnironment above
qqline(residuals)
```

Points which closely follow the line would suggest residuals which are approximately normally distributed. In the qq plot above we see that this is somewhat true. The VAST majority of the residuals are normally distributed except for some at the top of the distribution around the third quantile. This is likely because these are outline points which don't align well to the model and therefore have very high residual values. A more quantitative test or revisiting this later would be helpful to clarify it.

```{r, echo = FALSE}
shapiro.test(residuals)
```

This is the Shapiro-Wilk normality test. For normality we would be looking for the p-value to be high, however this p-value of 2.2e-16 is just about as low as it gets. This would mean that the residuals (at least as they are now including the outliers) fail the normality test.

## Checking for High Leverage Points

We can check the leverage using the hatvalues function.

```{r, echo = FALSE}
leverage_values = hatvalues(full_model)
plot(leverage_values, main = "Leverage Values", xlab = "Observation", ylab = "Leverage")

```

Immediately upon finding and plotting the leverage values we can see that there are a few high leverage points. One point is EXTREMELY high leverage. In order to better work with these points lets find the points specifically that we would consider high leverage. In this case take "high leverage" to mean at least twice the mean leverage of all points.

```{r, echo = FALSE}
high_leverage_points <- which(leverage_values > 2 * mean(leverage_values))
length(high_leverage_points)
```

With the points isolated we see that there are actually 185 points that are high leverage which is many more than the initial estimate you might have just by viewing the graph. These points are now saved to the vector `high_leverage_points` for easy access later. Plotting the high leverage points in a different color is helpful to see which they are as well:

```{r, echo = FALSE}
plot(leverage_values, main = "Leverage Values", xlab = "Observation", ylab = "Leverage")
points(high_leverage_points, leverage_values[high_leverage_points], col = "red", pch = 16)

```

The car corresponding to index 1036 has the largest leverage point of 0.337. Upon inspecting the dataframe this car has over 1.1 million miles, so it makes sense why it has such great influence. There are many other notable points looking at this plot which have leverages over the threshold of 2\*(k+1/n), so we should consider eliminating those in the future.

## Checking for Outlier Points in y

To check for outlier's in y we can use the residuals vs fitted value plot, but since we already used this plot above lets use jackknife residuals of our model, and test that for H_0: No outliers.

```{r, echo = FALSE}
jackknife_residuals <- rstudent(full_model)
jackknife_residuals[which.max(abs(jackknife_residuals))]
qt(.05/(2248*2), 2240)
```

The largest residual identified had a value of 13.2 which is way larger than our critical value of 4.25, so we reject the null hypothesis that none of our responses are outlying, and conclude that at least one of the used car sale prices is an outlier in the dataset. Since it is so high above the critical value it is likely that we have more than 1 outlier but that is not something we would be able to tell from here.

```{r, echo = FALSE}
plot(fitted_values, jackknife_residuals, main = "Jackknife Residuals vs Fitted Values", 
     xlab = "Fitted Values", ylab = "Jackknife Residuals")

abline(h = 0, col = "red", lty = 2)

text(fitted_values[abs(jackknife_residuals) > 4.25], 
     jackknife_residuals[abs(jackknife_residuals) > 4.25], 
     labels = which(abs(jackknife_residuals) > 4.25), col = "blue")
legend("topright", legend = c("Outliers"), col = c("blue"), pch = 1)
```

This graph helps us identify points likely to be outliers. They are compared on a graph of the jackknife residuals vs fitted values and the points which are likely to be outliers based on this are marked in blue. We will remove these in the next part and assess again.

## Checking for Influential Points

We will use Cook's Distance to check for influential points, as it determines how much the fitted values change when the ith data point is removed. Cook's Distance values larger than 1 will be considered influential, and values lesser than 0.5 will be considered not influential.

```{r, echo = FALSE}
cook_full <- cooks.distance(full_model)
plot(cook_full, ylab="Cook's Distance", main="Cook's Distance", type="l")
cook_full[which.max(cook_full)]
```

Observation 1036 is determined to be a very obvious outlier. This same index was previously determined to have very high leverage. Given these factors, it would be beneficial to remove this point from our dataset.

I will also renumber the indices in our dataset, because observation 1036 corresponds to index 620 following data cleanup. This will align the two values.

### Renumbering indices

```{r, echo = FALSE}
new_indices <- 1:nrow(data) 
rownames(data) <- new_indices
```

### Removing observation 620 (formerly observation 1036)

```{r, echo = FALSE}
data_2 <- data[-c(620, 706, 718, 1158, 1958, 1764, 1967, 1893), ]
```

### Reanalyzing fit of model and Cook's Distance without this point

```{r, echo = FALSE}
full_model_2 <- lm(Price~ . - Body_Type, data_2)
summary(full_model_2)
```

We see the adjusted R squared changed by about 0.0437 after removing these points.

```{r, echo = FALSE}
cook_full_2 <- cooks.distance(full_model_2) 
plot(cook_full_2, ylab="Cook's Distance", main="Cook's Distance", type="l")
cook_full_2[which.max(cook_full_2)]
```

Our new largest Cook's Distance is 0.05, which is much less than our threshold of 0.5. This tells us that our new dataset does not contain significantly influential points.

## Checking for Outlier Points in y (attempt 2)

We will look at the jackknife residuals of our new full model, and test againat the null hypothesis that none of our responses are outliers.

```{r, echo = FALSE}
new_jackknife_residuals <- rstudent(full_model_2)
new_jackknife_residuals[which.max(abs(new_jackknife_residuals))]
qt(.05/(2240*2), 2232)
```

Our largest jackknife residual of 5.135 is larger than our critical value of 4.25, so we reject the null hypothesis that none of our responses are outlying, and conclude that at least one of the used car sale prices is an outlier in the dataset.

```{r, echo = FALSE}
new_fitted_values = fitted(full_model_2)
plot(new_fitted_values, new_jackknife_residuals, main = "Jackknife Residuals vs Fitted Values", 
     xlab = "Fitted Values", ylab = "Jackknife Residuals")

abline(h = 0, col = "red", lty = 2)

text(new_fitted_values[abs(new_jackknife_residuals) > 4.25], 
     new_jackknife_residuals[abs(new_jackknife_residuals) > 4.25], 
     labels = which(abs(new_jackknife_residuals) > 4.25), col = "blue")
legend("topright", legend = c("Outliers"), col = c("blue"), pch = 1)
```

We have now identified an additional point, 1184 as an outlier. Lets drop that and check again.

```{r, echo = FALSE}
data_2 <- data_2[-c(1184), ]
full_model_2 <- lm(Price~ . - Body_Type, data_2)
new_jackknife_residuals <- rstudent(full_model_2)
new_jackknife_residuals[which.max(abs(new_jackknife_residuals))]
qt(.05/(2240*2), 2232)
```

We can see here that the jackknife residuals are approaching the critical value. It isnt quite under the critical value but it is remarkably close.

## Checking the structure of the model

```{r, echo = FALSE}
plot(full_model_2)
```

Having already dropped the points that were identified as outliers above all remaning points are well within the cook distance even if they do stray from the line.

## Exploring Possibility of a Box-Cox Transformation

```{r, echo = FALSE}
library(MASS)
boxcox(full_model_2, plotit=T)
boxcox(full_model_2, plotit=T, lambda=seq(0.1, 0.3, by=0.05))
```

As we can see in our plot, lambda = 1 is not within the range of plausible values. As a result, it would be good to perform a box-cox transformation on our model. Specifically, the center of our interval lies around 0.16 = (1/6), so a transformation of the response to the power of 1/6 seems plausible.

## Creating new model with transformation

```{r, echo = FALSE}
full_model_transformed <- lm(Price^(1/6)~ . - Body_Type, data_2)
summary(full_model_transformed)
```

Looking at this summary, we can see that our adjusted R-squared value increased significantly from .72 to 0.8. The significance of our predictors remained largely unchanged; however, the magnitudes of the betas are drastically different.

## Reperforming Model Diagnostics

### Checking for homoskedasticity

```{r, echo = FALSE}
plot(full_model_transformed)
```

When we revisit the plot of residuals vs fitted values, the previously seen crescent shape has turned into a more evenly scattered distribution, which is what we wanted to see. Observations 706 and 718 still have very large residuals. Overall, the distribution looks more homoskedastic.

### Checking normality assumption using qq-plots and S-W Normality Test

Looking at the QQ-Residuals plot, the vast majority of the residuals fall on the qq-line. However, a few in partifular stray far from this line, notable 1531.

```{r, echo = FALSE}
shapiro.test(resid(full_model_transformed))
```

When we reperform the S-W test, we once again receive an extremely low p-value, indicating that we should conclude that our data is still not normally distributed.

# Exploring Serial Correlation in the Errors

```{r, echo = FALSE}
library(nlme)
nlme <- gls(Price~ . - Body_Type, correlation=corARMA(p=1), data=data_2)
summary(nlme)
```

We see that the estimated value of rho is 0.07. To check if this is significant, we then compute its confidence interval:

```{r, echo = FALSE}
intervals(nlme)
```

Becasue 0 is not contained in the confidence interval for rho, there is evidence of serial correlation of the errors
